{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code is from kaggle, data is from Udemy \n",
    "# https://www.kaggle.com/nirajvermafcb/principal-component-analysis-explained\n",
    "# https://www.youtube.com/watch?v=n7npKX5zIWI&t=548s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic_Acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Ash_Alcanity</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total_Phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid_Phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color_Intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280</th>\n",
       "      <th>Proline</th>\n",
       "      <th>Customer_Segment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Alcohol  Malic_Acid   Ash  Ash_Alcanity  Magnesium  Total_Phenols  \\\n",
       "0    14.23        1.71  2.43          15.6        127           2.80   \n",
       "1    13.20        1.78  2.14          11.2        100           2.65   \n",
       "2    13.16        2.36  2.67          18.6        101           2.80   \n",
       "3    14.37        1.95  2.50          16.8        113           3.85   \n",
       "4    13.24        2.59  2.87          21.0        118           2.80   \n",
       "\n",
       "   Flavanoids  Nonflavanoid_Phenols  Proanthocyanins  Color_Intensity   Hue  \\\n",
       "0        3.06                  0.28             2.29             5.64  1.04   \n",
       "1        2.76                  0.26             1.28             4.38  1.05   \n",
       "2        3.24                  0.30             2.81             5.68  1.03   \n",
       "3        3.49                  0.24             2.18             7.80  0.86   \n",
       "4        2.69                  0.39             1.82             4.32  1.04   \n",
       "\n",
       "   OD280  Proline  Customer_Segment  \n",
       "0   3.92     1065                 1  \n",
       "1   3.40     1050                 1  \n",
       "2   3.17     1185                 1  \n",
       "3   3.45     1480                 1  \n",
       "4   2.93      735                 1  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('Wine.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Alcohol                 0\n",
       "Malic_Acid              0\n",
       "Ash                     0\n",
       "Ash_Alcanity            0\n",
       "Magnesium               0\n",
       "Total_Phenols           0\n",
       "Flavanoids              0\n",
       "Nonflavanoid_Phenols    0\n",
       "Proanthocyanins         0\n",
       "Color_Intensity         0\n",
       "Hue                     0\n",
       "OD280                   0\n",
       "Proline                 0\n",
       "Customer_Segment        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.423e+01, 1.710e+00, 2.430e+00, ..., 1.040e+00, 3.920e+00,\n",
       "         1.065e+03],\n",
       "        [1.320e+01, 1.780e+00, 2.140e+00, ..., 1.050e+00, 3.400e+00,\n",
       "         1.050e+03],\n",
       "        [1.316e+01, 2.360e+00, 2.670e+00, ..., 1.030e+00, 3.170e+00,\n",
       "         1.185e+03],\n",
       "        ...,\n",
       "        [1.327e+01, 4.280e+00, 2.260e+00, ..., 5.900e-01, 1.560e+00,\n",
       "         8.350e+02],\n",
       "        [1.317e+01, 2.590e+00, 2.370e+00, ..., 6.000e-01, 1.620e+00,\n",
       "         8.400e+02],\n",
       "        [1.413e+01, 4.100e+00, 2.740e+00, ..., 6.100e-01, 1.600e+00,\n",
       "         5.600e+02]]),\n",
       " array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3], dtype=int64))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = dataset.iloc[:,:-1].values\n",
    "y = dataset.iloc[:,-1].values\n",
    "X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset into the Training and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "\n",
    "* Standardization is all about scaling your data in such a way that all the variables and their values lie within a similar range\n",
    "\n",
    "\n",
    "*  if there is a significant difference in the scale between the features of the dataset; for example, \n",
    "   one feature ranges in values between 0 and 1 and another between 100 and 1,000. PCA is very sensitive \n",
    "   to the relative ranges of the original features. We can apply z-score standardization to get all features \n",
    "   into the same scale by using Scikit-learn StandardScaler() class which is in the preprocessing submodule in Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((142, 13), (36, 13))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "X_train.shape, X_test.shape\n",
    "\n",
    "#when u are applying your standard scaler object to your training set u have to fit the object of the training set and then\n",
    "#transform it, and when coming to test set we only transform and don't fit the object b'cause it's already fitted to the \n",
    "#training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigendecomposition - Computing Eigenvectors and Eigenvalues\n",
    "* The eigenvectors and eigenvalues of a covariance (or correlation) matrix represent the “core” of a PCA: The eigenvectors    (principal components) determine the directions of the new feature space, and the eigenvalues determine their magnitude. In      other words, the eigenvalues explain the variance of the data along the new feature axes.\n",
    "\n",
    "### Covariance Matrix \n",
    "* The classic approach to PCA is to perform the eigendecomposition on the covariance matrix , which is a matrix where each element represents the covariance between two features. The covariance between two features is calculated as follows:\n",
    "\n",
    "Cov(X,Y)=∑(xi−x¯)(yi−y¯)/N−1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covariance matrix \n",
      "[[ 1.0070922   0.10158986  0.26809509 -0.27516974  0.23703425  0.31671433\n",
      "   0.27851441 -0.10086553  0.1305509   0.57003979 -0.06801958  0.05747774\n",
      "   0.62420905]\n",
      " [ 0.10158986  1.0070922   0.17469631  0.26867072 -0.03184697 -0.32482824\n",
      "  -0.42013761  0.33304303 -0.22082499  0.30464878 -0.54532022 -0.39388534\n",
      "  -0.18750673]\n",
      " [ 0.26809509  0.17469631  1.0070922   0.44913101  0.20279733  0.16173403\n",
      "   0.09405082  0.15934325  0.01757216  0.26297426 -0.09527385 -0.01162687\n",
      "   0.19229708]\n",
      " [-0.27516974  0.26867072  0.44913101  1.0070922  -0.10161795 -0.32802847\n",
      "  -0.4026876   0.38895703 -0.21705802  0.01275967 -0.25428449 -0.30031831\n",
      "  -0.43572181]\n",
      " [ 0.23703425 -0.03184697  0.20279733 -0.10161795  1.0070922   0.18356106\n",
      "   0.16698341 -0.31275131  0.26207895  0.15237759  0.04270468  0.03007895\n",
      "   0.34142812]\n",
      " [ 0.31671433 -0.32482824  0.16173403 -0.32802847  0.18356106  1.0070922\n",
      "   0.86413313 -0.4576375   0.62739301 -0.06254654  0.45572406  0.71439803\n",
      "   0.53185581]\n",
      " [ 0.27851441 -0.42013761  0.09405082 -0.4026876   0.16698341  0.86413313\n",
      "   1.0070922  -0.58037954  0.66722271 -0.20619231  0.5913848   0.80281821\n",
      "   0.54364005]\n",
      " [-0.10086553  0.33304303  0.15934325  0.38895703 -0.31275131 -0.4576375\n",
      "  -0.58037954  1.0070922  -0.32612799  0.19408361 -0.3308522  -0.51346418\n",
      "  -0.33294415]\n",
      " [ 0.1305509  -0.22082499  0.01757216 -0.21705802  0.26207895  0.62739301\n",
      "   0.66722271 -0.32612799  1.0070922  -0.03153532  0.31249257  0.50922193\n",
      "   0.35522199]\n",
      " [ 0.57003979  0.30464878  0.26297426  0.01275967  0.15237759 -0.06254654\n",
      "  -0.20619231  0.19408361 -0.03153532  1.0070922  -0.54975259 -0.45627551\n",
      "   0.31188331]\n",
      " [-0.06801958 -0.54532022 -0.09527385 -0.25428449  0.04270468  0.45572406\n",
      "   0.5913848  -0.3308522   0.31249257 -0.54975259  1.0070922   0.60932504\n",
      "   0.22836735]\n",
      " [ 0.05747774 -0.39388534 -0.01162687 -0.30031831  0.03007895  0.71439803\n",
      "   0.80281821 -0.51346418  0.50922193 -0.45627551  0.60932504  1.0070922\n",
      "   0.30649834]\n",
      " [ 0.62420905 -0.18750673  0.19229708 -0.43572181  0.34142812  0.53185581\n",
      "   0.54364005 -0.33294415  0.35522199  0.31188331  0.22836735  0.30649834\n",
      "   1.0070922 ]]\n"
     ]
    }
   ],
   "source": [
    "X_mean = np.mean(X_train, axis=0)\n",
    "cov_mat = (X_train - X_mean).T.dot((X_train - X_mean)) / (X_train.shape[0]-1)\n",
    "print('Covariance matrix \\n%s' %cov_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covariance matrix \n",
      "[[ 1.0070922   0.10158986  0.26809509 -0.27516974  0.23703425  0.31671433\n",
      "   0.27851441 -0.10086553  0.1305509   0.57003979 -0.06801958  0.05747774\n",
      "   0.62420905]\n",
      " [ 0.10158986  1.0070922   0.17469631  0.26867072 -0.03184697 -0.32482824\n",
      "  -0.42013761  0.33304303 -0.22082499  0.30464878 -0.54532022 -0.39388534\n",
      "  -0.18750673]\n",
      " [ 0.26809509  0.17469631  1.0070922   0.44913101  0.20279733  0.16173403\n",
      "   0.09405082  0.15934325  0.01757216  0.26297426 -0.09527385 -0.01162687\n",
      "   0.19229708]\n",
      " [-0.27516974  0.26867072  0.44913101  1.0070922  -0.10161795 -0.32802847\n",
      "  -0.4026876   0.38895703 -0.21705802  0.01275967 -0.25428449 -0.30031831\n",
      "  -0.43572181]\n",
      " [ 0.23703425 -0.03184697  0.20279733 -0.10161795  1.0070922   0.18356106\n",
      "   0.16698341 -0.31275131  0.26207895  0.15237759  0.04270468  0.03007895\n",
      "   0.34142812]\n",
      " [ 0.31671433 -0.32482824  0.16173403 -0.32802847  0.18356106  1.0070922\n",
      "   0.86413313 -0.4576375   0.62739301 -0.06254654  0.45572406  0.71439803\n",
      "   0.53185581]\n",
      " [ 0.27851441 -0.42013761  0.09405082 -0.4026876   0.16698341  0.86413313\n",
      "   1.0070922  -0.58037954  0.66722271 -0.20619231  0.5913848   0.80281821\n",
      "   0.54364005]\n",
      " [-0.10086553  0.33304303  0.15934325  0.38895703 -0.31275131 -0.4576375\n",
      "  -0.58037954  1.0070922  -0.32612799  0.19408361 -0.3308522  -0.51346418\n",
      "  -0.33294415]\n",
      " [ 0.1305509  -0.22082499  0.01757216 -0.21705802  0.26207895  0.62739301\n",
      "   0.66722271 -0.32612799  1.0070922  -0.03153532  0.31249257  0.50922193\n",
      "   0.35522199]\n",
      " [ 0.57003979  0.30464878  0.26297426  0.01275967  0.15237759 -0.06254654\n",
      "  -0.20619231  0.19408361 -0.03153532  1.0070922  -0.54975259 -0.45627551\n",
      "   0.31188331]\n",
      " [-0.06801958 -0.54532022 -0.09527385 -0.25428449  0.04270468  0.45572406\n",
      "   0.5913848  -0.3308522   0.31249257 -0.54975259  1.0070922   0.60932504\n",
      "   0.22836735]\n",
      " [ 0.05747774 -0.39388534 -0.01162687 -0.30031831  0.03007895  0.71439803\n",
      "   0.80281821 -0.51346418  0.50922193 -0.45627551  0.60932504  1.0070922\n",
      "   0.30649834]\n",
      " [ 0.62420905 -0.18750673  0.19229708 -0.43572181  0.34142812  0.53185581\n",
      "   0.54364005 -0.33294415  0.35522199  0.31188331  0.22836735  0.30649834\n",
      "   1.0070922 ]]\n"
     ]
    }
   ],
   "source": [
    "cov_mat = np.cov(X_train.T)    # another simple approach for Covariance matrix without typing covariance formula\n",
    "print('Covariance matrix \\n%s' %cov_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvectors \n",
      "[[ 0.12959991  0.49807323 -0.1383815   0.24199813  0.17279973  0.17873289\n",
      "  -0.09000534 -0.08850175 -0.26402263  0.48169166 -0.43303897 -0.2793571\n",
      "   0.11125519]\n",
      " [-0.24464064  0.23168482  0.08422378  0.03501265 -0.58431883  0.50041745\n",
      "  -0.44755409  0.02912494  0.05505047  0.08986837  0.18860336  0.13627368\n",
      "  -0.14108829]\n",
      " [-0.01018912  0.31496874  0.63988217  0.00978418  0.26362755  0.16064973\n",
      "   0.06751248 -0.1213575   0.01688933 -0.37909702  0.0325249  -0.32898212\n",
      "  -0.35109208]\n",
      " [-0.24051579 -0.02321825  0.62195017 -0.0922443   0.02622374 -0.0559492\n",
      "   0.24334226  0.08600336 -0.05553175  0.51955024  0.13288449  0.22277751\n",
      "   0.36816801]\n",
      " [ 0.12649451  0.25841951  0.02997765 -0.82788074  0.08753303 -0.07810127\n",
      "  -0.30855505  0.09468425  0.06758476 -0.07256418 -0.26176222  0.13059365\n",
      "   0.13263758]\n",
      " [ 0.38944115  0.1006849   0.17115651  0.16969861 -0.16632648 -0.0217019\n",
      "   0.08597346 -0.36183225 -0.3119664  -0.21081573 -0.21572363  0.65199927\n",
      "  -0.01582547]\n",
      " [ 0.42757808  0.02097952  0.12464239  0.12346501 -0.10686668  0.04491501\n",
      "   0.06064567  0.85712436 -0.14445235 -0.03557095 -0.02588866  0.02672564\n",
      "  -0.10326349]\n",
      " [-0.30505669  0.0399057   0.17100464  0.39521252  0.10735201 -0.45316488\n",
      "  -0.56383754  0.16174139  0.03779118 -0.22998878 -0.21323232  0.08341792\n",
      "   0.22848836]\n",
      " [ 0.30775255  0.06746036  0.15155321 -0.06413801 -0.52323021 -0.58035398\n",
      "  -0.08171916 -0.19425527 -0.05533473  0.17940325  0.15164099 -0.38787224\n",
      "  -0.0998717 ]\n",
      " [-0.11027186  0.53087111 -0.14612801  0.10827243 -0.02931966 -0.27220968\n",
      "   0.31061932  0.09718855  0.5762829   0.101621   -0.01409547  0.28818112\n",
      "  -0.25763558]\n",
      " [ 0.30710508 -0.27161729  0.10692701  0.05295673  0.38019616 -0.00633457\n",
      "  -0.42574367 -0.09265306  0.21977435  0.42699892  0.13285265  0.19455725\n",
      "  -0.44397073]\n",
      " [ 0.37636185 -0.16071181  0.17144241  0.14487111 -0.14033771  0.25220324\n",
      "  -0.00506355 -0.1046235   0.6444231  -0.07102473 -0.23890199 -0.15064551\n",
      "   0.43333198]\n",
      " [ 0.2811085   0.36547344 -0.13178214  0.07973043  0.24774953  0.01400986\n",
      "  -0.13851706 -0.04211232 -0.03100444 -0.11178512  0.70645439  0.04073879\n",
      "   0.40859824]]\n",
      "\n",
      "Eigenvalues \n",
      "[4.82894083 2.52920254 1.40778607 0.97170248 0.81772614 0.64269609\n",
      " 0.53904343 0.09124383 0.16401706 0.32677915 0.30227988 0.22672631\n",
      " 0.24405475]\n"
     ]
    }
   ],
   "source": [
    "# Next, we perform an eigen decomposition on the covariance matrix:\n",
    "\n",
    "eig_vals, eig_vecs = np.linalg.eig(cov_mat)\n",
    "\n",
    "print('Eigenvectors \\n%s' %eig_vecs)\n",
    "print('\\nEigenvalues \\n%s' %eig_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4.8289408339225215,\n",
       "  array([ 0.12959991, -0.24464064, -0.01018912, -0.24051579,  0.12649451,\n",
       "          0.38944115,  0.42757808, -0.30505669,  0.30775255, -0.11027186,\n",
       "          0.30710508,  0.37636185,  0.2811085 ])),\n",
       " (2.529202544186417,\n",
       "  array([ 0.49807323,  0.23168482,  0.31496874, -0.02321825,  0.25841951,\n",
       "          0.1006849 ,  0.02097952,  0.0399057 ,  0.06746036,  0.53087111,\n",
       "         -0.27161729, -0.16071181,  0.36547344])),\n",
       " (1.4077860734697651,\n",
       "  array([-0.1383815 ,  0.08422378,  0.63988217,  0.62195017,  0.02997765,\n",
       "          0.17115651,  0.12464239,  0.17100464,  0.15155321, -0.14612801,\n",
       "          0.10692701,  0.17144241, -0.13178214])),\n",
       " (0.9717024794763274,\n",
       "  array([ 0.24199813,  0.03501265,  0.00978418, -0.0922443 , -0.82788074,\n",
       "          0.16969861,  0.12346501,  0.39521252, -0.06413801,  0.10827243,\n",
       "          0.05295673,  0.14487111,  0.07973043])),\n",
       " (0.817726141338358,\n",
       "  array([ 0.17279973, -0.58431883,  0.26362755,  0.02622374,  0.08753303,\n",
       "         -0.16632648, -0.10686668,  0.10735201, -0.52323021, -0.02931966,\n",
       "          0.38019616, -0.14033771,  0.24774953])),\n",
       " (0.6426960928882302,\n",
       "  array([ 0.17873289,  0.50041745,  0.16064973, -0.0559492 , -0.07810127,\n",
       "         -0.0217019 ,  0.04491501, -0.45316488, -0.58035398, -0.27220968,\n",
       "         -0.00633457,  0.25220324,  0.01400986])),\n",
       " (0.539043433256971,\n",
       "  array([-0.09000534, -0.44755409,  0.06751248,  0.24334226, -0.30855505,\n",
       "          0.08597346,  0.06064567, -0.56383754, -0.08171916,  0.31061932,\n",
       "         -0.42574367, -0.00506355, -0.13851706])),\n",
       " (0.09124383308608718,\n",
       "  array([-0.08850175,  0.02912494, -0.1213575 ,  0.08600336,  0.09468425,\n",
       "         -0.36183225,  0.85712436,  0.16174139, -0.19425527,  0.09718855,\n",
       "         -0.09265306, -0.1046235 , -0.04211232])),\n",
       " (0.16401706132420635,\n",
       "  array([-0.26402263,  0.05505047,  0.01688933, -0.05553175,  0.06758476,\n",
       "         -0.3119664 , -0.14445235,  0.03779118, -0.05533473,  0.5762829 ,\n",
       "          0.21977435,  0.6444231 , -0.03100444])),\n",
       " (0.32677915004292307,\n",
       "  array([ 0.48169166,  0.08986837, -0.37909702,  0.51955024, -0.07256418,\n",
       "         -0.21081573, -0.03557095, -0.22998878,  0.17940325,  0.101621  ,\n",
       "          0.42699892, -0.07102473, -0.11178512])),\n",
       " (0.30227987874900986,\n",
       "  array([-0.43303897,  0.18860336,  0.0325249 ,  0.13288449, -0.26176222,\n",
       "         -0.21572363, -0.02588866, -0.21323232,  0.15164099, -0.01409547,\n",
       "          0.13285265, -0.23890199,  0.70645439])),\n",
       " (0.2267263071015333,\n",
       "  array([-0.2793571 ,  0.13627368, -0.32898212,  0.22277751,  0.13059365,\n",
       "          0.65199927,  0.02672564,  0.08341792, -0.38787224,  0.28818112,\n",
       "          0.19455725, -0.15064551,  0.04073879])),\n",
       " (0.24405475271792992,\n",
       "  array([ 0.11125519, -0.14108829, -0.35109208,  0.36816801,  0.13263758,\n",
       "         -0.01582547, -0.10326349,  0.22848836, -0.0998717 , -0.25763558,\n",
       "         -0.44397073,  0.43333198,  0.40859824]))]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a list of (eigenvalue, eigenvector) tuples\n",
    "\n",
    "eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
    "eig_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues in descending order:\n",
      "4.8289408339225215\n",
      "2.529202544186417\n",
      "1.4077860734697651\n",
      "0.9717024794763274\n",
      "0.817726141338358\n",
      "0.6426960928882302\n",
      "0.539043433256971\n",
      "0.32677915004292307\n",
      "0.30227987874900986\n",
      "0.24405475271792992\n",
      "0.2267263071015333\n",
      "0.16401706132420635\n",
      "0.09124383308608718\n"
     ]
    }
   ],
   "source": [
    "# Sort the (eigenvalue, eigenvector) tuples from high to low\n",
    "\n",
    "eig_pairs.sort(key = lambda x:x[0], reverse = True)\n",
    "\n",
    "# Visually confirm that the list is correctly sorted by decreasing eigenvalues\n",
    "\n",
    "print('Eigenvalues in descending order:')\n",
    "for i in eig_pairs:\n",
    "    print(i[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explained Variance:\n",
    "After sorting the eigenpairs, the next question is \"how many principal components are we going to choose for our new feature subspace?\" A useful measure is the so-called \"explained variance,\" which can be calculated from the eigenvalues. The explained variance tells us how much information (variance) can be attributed to each of the principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 36.88410929,  56.20250359,  66.9553658 ,  74.37736199,\n",
       "        80.62326588,  85.53226638,  89.6495537 ,  92.14553746,\n",
       "        94.45439244,  96.31851596,  98.05028244,  99.30306715,\n",
       "       100.        ])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tot = sum(eig_vals)\n",
    "var_exp = [(i / tot)*100 for i in sorted(eig_vals, reverse=True)]\n",
    "cum_var_exp = np.cumsum(var_exp)\n",
    "cum_var_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the above array we see that the first feature explains roughly 36.88% of the variance within our data set \n",
    "# while the second explain 56.20 and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Cummulative explained variance')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2a71d155630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(cum_var_exp)\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cummulative explained variance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above plot shows almost 90% variance by the first 12 components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((142, 12), (36, 12))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=12, random_state =42)\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "\n",
    "X_train.shape, X_test.shape\n",
    "\n",
    "# here n_components is the final number of extracted features in your new data set \n",
    "\n",
    "# How many n_components or extracted features u need to choose ? (to reduce the dimentionality of the dataset)\n",
    "\n",
    "# Lets start with 2 pricipal components(i.e, 2 exracted features) and see the result u get \n",
    "# If we get good results with 2 well we'll be able to visualize the training set and test set result in 2 dimensions.\n",
    "# If we get poor results and we see on the graph that we cant separate the three classes(i.e,our dependent variable has \n",
    "# 3 classes (1,2,3) ) properly then we go for higher number of principal components like 3,4,... and at some point we'll get\n",
    "# *** some extracted features that explian well enough \"variance\" which is exactly what PCA about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Logistic Regression model on the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(random_state = 0)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3, 2, 1, 2, 2, 1, 3, 2, 2, 3, 3, 1, 2, 3, 2, 1, 1, 2, 1, 2, 1,\n",
       "       1, 2, 2, 2, 2, 2, 2, 3, 1, 1, 2, 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3, 2, 1, 2, 2, 1, 3, 2, 2, 3, 3, 1, 2, 3, 2, 1, 1, 2, 1, 2, 1,\n",
       "       1, 2, 2, 2, 2, 2, 2, 3, 1, 1, 2, 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1\n",
       "0  1  1\n",
       "1  3  3\n",
       "2  2  2\n",
       "3  1  1\n",
       "4  2  2"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data = [y_pred,y_test])\n",
    "df.T.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Accuracy with Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14  0  0]\n",
      " [ 0 16  0]\n",
      " [ 0  0  6]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "y_pred = classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So we got 100% accuracy witn n_components = 12"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
